# -*- coding: utf-8 -*-
"""Generate and Compare LLM Outputs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HcMqzKGVVgblLSAeRSuXFg8ImC3NinvY

# Setup
"""

#import API keys and mount google drive
from google.colab import drive
from google.colab import userdata
!pip install --upgrade openai
#make sure we have openAI and Gemini access tokens before proceeding (key icon on left)
import os
from openai import OpenAI
import google.generativeai as genai
# Pass the API key to Open AI
os.environ['OPEN_AI_API_KEY'] = userdata.get("OPEN_AI_PROJECT_KEY")
client = OpenAI(api_key=os.environ.get("OPEN_AI_API_KEY"))
# Pass the API key to Gemini
os.environ['GEMINI_API_KEY'] = userdata.get("GEMINI_PROJECT_KEY")
# Pass the API key to Grok
os.environ['GROK_API_KEY'] = userdata.get("GROK_PROJECT_KEY")
# Pass the API key to SERP
os.environ['SERP_API_KEY'] = userdata.get("SERP_PROJECT_KEY")
#mount drive
drive.mount('/content/drive')

## Import Data - imports the data as an excel file from the Google Sheet containing the text you want to feed to the LLM(s)

import gdown
import pandas as pd


#ALL 6000 search queries (1000 for each country - USA, Germany, S Korea, Russia, Turkey, China)
#this is Ulysses' "ushmm_data_with_articles" spreadsheet that includes the ushmmm article link and text for each query
url_ALL = 'https://docs.google.com/spreadsheets/d/1i6PRehieuZKl2Zk7EoJFjwRuS4e-NR8mtBAbTt6rZ4c/edit?gid=525940975#gid=525940975'
export_url_ALL= f"{url_ALL}"+"/export?format=xlsx"
temp_file_ALL = "file_ALL.xlsx"
gdown.download(export_url_ALL, temp_file_ALL, quiet=False, fuzzy=True)

# Load the Excel file into pandas
df_ALL = pd.read_excel(temp_file_ALL)

# Display the DataFrame
print(df_ALL.head())

#If that does not work, simply load the excel file from the directory
df_ALL = pd.read_excel('ushmm_data_with_articles.xlsx')
# Display the DataFrame
print(df_ALL.head())

"""# Initialize LLMs and define functions to query them"""

#load API keys
OPENAI_API_KEY = userdata.get('OPEN_AI_PROJECT_KEY')
GOOGLE_API_KEY = userdata.get('GEMINI_PROJECT_KEY')
GROK_API_KEY = userdata.get('GROK_PROJECT_KEY')

#GPT

from typing import List, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
import json
import time

# Import Colab Secrets userdata module
from google.colab import userdata
import os
from openai import OpenAI


# Pass the API key to Open AI
client = OpenAI(api_key=userdata.get('OPEN_AI_PROJECT_KEY'))

def query_gpt4o(prompt: str) -> str:
  try:
    time.sleep(0.13)
    response = client.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model="gpt-4o",
    )
    return response.choices[0].message.content
  except Exception as e:
    return f"GPT-4o Error: {str(e)}"

  #alternative querying functions
def base_gpt(input, model):
    response = client.chat.completions.create(
        messages=[
            {"role": "user", "content": input,}],
        model=model,
    )
    output_dict = {
        "model": model,
        "output": response.choices[0].message.content
    }
    return response.choices[0].message.content

def generate_LLM_response(row) -> List[str]:
    #Let's start the the fewest assumptions
    input_data = row['Top queries']
    response = base_gpt(input_data, "gpt-4o")
    return response

#Gemini

from typing import List, Dict
import json
import os
import requests
import google.generativeai as genai
from google.colab import userdata
from openai import OpenAI

# Initialize Gemini
genai.configure(api_key=GOOGLE_API_KEY)

# Query Gemini 2.0 Flash
def query_gemini(prompt: str) -> str:
    model = genai.GenerativeModel(
    'gemini-2.0-flash',
    generation_config=genai.GenerationConfig(
        max_output_tokens=2000,
        temperature=0.9,
    ))
    try:
        time.sleep(4.1)  # ~4 seconds delay to stay under 2.0 flash rate limit 15 requests/minute
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Error: {str(e)}"

#Grok

# Query Grok latest version
# Initialize Grok Client
grok_client = OpenAI(
    api_key=GROK_API_KEY,
    base_url="https://api.x.ai/v1",
)

# Function to Query Grok
def query_grok(prompt: str) -> str:
    try:
        time.sleep(0.13) # ~1/8 seconds delay to stay under grok-2-1212 rate limit of 8 requests/second
        response = grok_client.chat.completions.create(
            model="grok-2-latest",  # Use the latest Grok model
            messages=[{"role": "user", "content": prompt}],
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Grok API Error: {str(e)}"

"""#Generate LLM Responses"""

# Generate LLM Responses=
def generate_LLM_responses(row) -> Dict[str, str]:
    prompt = row['Top queries']

    return {
        "GPT-4o": query_gpt4o(prompt),
        "Gemini": query_gemini(prompt),
        "Grok": query_grok(prompt)
    }

import pandas as pd

# Process DataFrame and Add LLM Responses
def generate_LLM_responses_df(df):
    # Apply the function to each row and create new columns
    df[['GPT-4o Response', 'Gemini Response', 'Grok Response']] = df['Top queries'].apply(
        lambda prompt: pd.Series(generate_LLM_responses({"Top queries": prompt}))
    )
    return df

us_search_responses_df = generate_LLM_responses_df(df_ALL[:1000])
print(us_search_responses_df.head())

"""# Import Cleaned Data set (optional)"""

import pandas as pd
import gdown

#If we want to specifically run the comparison tests on a different data set of queries and responses (such as outputs that have already been generated), import it here
# e.g. Cleaned data set of 776 US Queries with no Non-English or otherwise erroneous responses

#Load the file from a google doc link
url_clean = 'https://docs.google.com/spreadsheets/d/1AZPIbb5rfCnyjq6L4ukq3E95DrDqeoNX/edit?gid=1831836360#gid=1831836360'
export_url_clean= f"{url_clean}"+"/export?format=xlsx"
temp_file_clean = "file_clean.xlsx"
gdown.download(export_url_clean, temp_file_clean, quiet=False, fuzzy=True)

# Load the Excel file into pandas
df_clean = pd.read_excel(temp_file_clean)

#Load the cleaned file from the directory
#df_clean = pd.read_excel('(Clean) US Queries with LLM Responses and Composite Scores.xlsx')
#df_raw = pd.read_excel('(Raw) US Queries with LLM Responses and Composite Scores.xlsx')

# Display the DataFrame
print(df_clean.head())
#print(df_raw.head())
print(len(df_clean))
#print(len(df_raw))
us_search_responses_df = df_clean
#us_search_responses_df = df_raw

"""# Define functions for obtaining similarity and readability metrics

READABILITY METRICS
"""

#Word Count
import re
def word_count(text):
    #Returns the number of words in a given text using regex
    words = re.findall(r'\b\w+\b', text)
    return len(words)

#Flesch Reading Score
def count_sentences(text):
    """Counts the number of sentences based on punctuation."""
    return max(1, len(re.findall(r'[.!?]', text)))  # Avoid zero sentences

def count_words(text):
    """Counts the number of words in the text."""
    return max(1, len(text.split()))  # Avoid division by zero

def count_syllables(word):
    """Counts the number of syllables in a word using regex heuristics."""
    word = word.lower()
    syllables = re.findall(r'[aeiouy]+', word)  # Matches vowel clusters
    if word.endswith(("es", "ed")) and len(syllables) > 1:
        syllables.pop()  # Discount silent syllables
    return max(1, len(syllables))

def count_total_syllables(text):
    """Counts the total number of syllables in the text."""
    words = re.findall(r'\b\w+\b', text)  # Extract words
    return sum(count_syllables(word) for word in words)

def flesch_reading_ease(text):
    """Calculates the Flesch Reading Ease Score."""
    words = count_words(text)
    sentences = count_sentences(text)
    syllables = count_total_syllables(text)

    asl = words / sentences  # Average Sentence Length
    asw = syllables / words  # Average Syllables per Word

    score = 206.835 - (1.015 * asl) - (84.6 * asw)
    return round(score, 2)

#Coleman Liau Index
def count_letters(text):
    """Counts the number of letters (A-Z, a-z) in the text."""
    return sum(c.isalpha() for c in text)

def count_sentences(text):
    """Counts the number of sentences based on punctuation."""
    return max(1, len(re.findall(r'[.!?]', text)))  # Avoid zero sentences

def coleman_liau_index(text):
    """Calculates the Coleman-Liau Index for readability assessment."""
    letters = count_letters(text)
    words = count_words(text)
    sentences = count_sentences(text)

    L = (letters / words) * 100  # Average letters per 100 words
    S = (sentences / words) * 100  # Average sentences per 100 words

    CLI = 0.0588 * L - 0.296 * S - 15.8
    return round(CLI, 2)

"""SIMILARITY METRICS"""

#Cosine Similarity

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
# Define Get Embedding Function
def get_embedding(text, model="text-embedding-3-small"):
   text = text.replace("\n", " ")
   return client.embeddings.create(input = text, model=model).data[0].embedding

# Example Usage

# Create A text string
text = "Hello, I am a member of a Digital Humanities Lab"

# Apply the function to the text and inspect the output
print(get_embedding(text))

def cosine_similarity_text(text1, text2):
    # Generate Embeeddings for the text
    embedding1 = get_embedding(text1)
    embedding2 = get_embedding(text2)

    # Convert embeddings to numpy arrays
    embedding1 = np.array(embedding1).reshape(1, -1)
    embedding2 = np.array(embedding2).reshape(1, -1)

    # Calculate cosine similarity
    similarity = cosine_similarity(embedding1, embedding2)[0][0]

    # Display the result
    print(f"Cosine similarity: {similarity}")

def cosine_similarity_text_2(text1, text2):
    #Calculate cosine similarity between two long text strings using TF-IDF
    vectorizer = TfidfVectorizer().fit_transform([text1, text2])
    vectors = vectorizer.toarray()
    return cosine_similarity(vectors)[0, 1]

#Jaccard Index

from nltk.util import ngrams

def jaccard_index(text1, text2, n=3):
    """Calculate Jaccard index using n-grams for better comparison of longer texts."""
    set1 = set(ngrams(text1.split(), n))
    set2 = set(ngrams(text2.split(), n))

    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))

    return intersection / union if union != 0 else 0

"""# Apply readability/similarity functions to obtain composite similarity scores"""

#This function calculates the individual metrics (cosine similarity, jaccard index, flesch score, coleman-liau) individually
def process_text_comparisons(df):

    # Ensure required columns exist
    required_columns = ["ushmm_article", "GPT-4o Response", "Gemini Response", "Grok Response"]
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"CSV must contain the columns: {required_columns}")

    # Compute Readability and Word Count
    for col in required_columns:
        df[f"{col} - Word Count"] = df[col].apply(count_words)
        df[f"{col} - Flesch Score"] = df[col].apply(flesch_reading_ease)
        df[f"{col} - Coleman-Liau"] = df[col].apply(coleman_liau_index)

    # Pairwise Comparisons
    pairings = [
        ("ushmm_article", "GPT-4o Response"),
        ("ushmm_article", "Gemini Response"),
        ("ushmm_article", "Grok Response"),
        ("GPT-4o Response", "Gemini Response"),
        ("GPT-4o Response", "Grok Response"),
        ("Gemini Response", "Grok Response"),
    ]

    for col1, col2 in pairings:
        df[f"{col1}/{col2} - Cosine Sim"] = df.apply(lambda row: cosine_similarity_text_2(row[col1], row[col2]), axis=1)
        df[f"{col1}/{col2} - Jaccard Index"] = df.apply(lambda row: jaccard_index(row[col1], row[col2]), axis=1)

    return df

#calculates word count only
def process_word_count(df):

    # Ensure required columns exist
    required_columns = ["ushmm_article", "GPT-4o Response", "Gemini Response", "Grok Response"]
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"CSV must contain the columns: {required_columns}")

    # Compute Readability and Word Count
    for col in required_columns:
        df[f"{col} - Word Count"] = df[col].apply(count_words)
        df[f"{col} - Flesch Score"] = df[col].apply(flesch_reading_ease)
        df[f"{col} - Coleman-Liau"] = df[col].apply(coleman_liau_index)
    return df

#Function for computing the composite similarity between two texts

#Takes in four input similarity metrics
# 1. cosine similarity between two texts
# 2. jaccard index between two texts
# 3. difference in flesch reading score between two texts
# 4. differences in coleman-liau score between two texts
# each of the values are normalized to a [0, 1] scale in which 0 is least similar and 1 is identical
# the values are combined into a weighted average score based on the weights assigned in the constant variables below
COSINE_SIMILARITY_WEIGHT = 0.4
JACCARD_INDEX_WEIGHT = 0.3
FLESCH_SCORE_WEIGHT = 0.15
COLEMAN_LIAU_WEIGHT = 0.15
def composite_similarity(text1, text2):
    cos = cosine_similarity_text_2(text1, text2)
    jac = jaccard_index(text1, text2)
    flesch_sim = 1 - max(0, 1 - abs(flesch_reading_ease(text1) - flesch_reading_ease(text2)) / 100)
    cli_sim = 1 - abs(coleman_liau_index(text1) - coleman_liau_index(text2)) / 20
    return round(0.4 * cos + 0.3 * jac + 0.15 * flesch_sim + 0.15 * cli_sim, 4)

#Function for taking in a df of USHMM, GPT, Gemini, and Grok responses to queries  and computing the composite similarities between each responses for each query
def compute_composite_similarity_scores(df):
    """
    Prepares a DataFrame by selecting specific columns, adding empty columns for similarity scores,
    and computing similarity between specified pairs of text fields.

    Args:
        df (pd.DataFrame): The input DataFrame.

    Returns:
        pd.DataFrame: A new DataFrame with similarity columns populated.
    """
    import pandas as pd

    # Define the columns to keep
    columns_to_keep = [
        'id', 'location', 'Top queries', 'ushmm_article',
        'GPT-4o Response', 'Gemini Response', 'Grok Response'
    ]

    # Define the new empty columns to add
    new_columns = [
        'USHMM-GPT Similarity', 'USHMM-Gemini Similarity', 'USHMM-Grok Similarity',
        'GPT-Gemini Similarity', 'GPT-Grok Similarity', 'Gemini-Grok Similarity',
        'Notes'
    ]

    # Filter the DataFrame to keep only the required columns
    df_new = df[columns_to_keep].copy()

    # Add the new empty columns
    for col in new_columns:
        df_new[col] = ""

    # Define the text column pairs for which similarity is computed
    pairs = [
        ("ushmm_article", "GPT-4o Response", "USHMM-GPT Similarity"),
        ("ushmm_article", "Gemini Response", "USHMM-Gemini Similarity"),
        ("ushmm_article", "Grok Response", "USHMM-Grok Similarity"),
        ("GPT-4o Response", "Gemini Response", "GPT-Gemini Similarity"),
        ("GPT-4o Response", "Grok Response", "GPT-Grok Similarity"),
        ("Gemini Response", "Grok Response", "Gemini-Grok Similarity")
    ]

    # Compute the similarity for each specified pair
    for col1, col2, new_col in pairs:
        df_new[new_col] = df_new.apply(
            lambda row: composite_similarity(row[col1], row[col2]),
            axis=1
        )

    return df_new

us_search_responses_df_with_composite = compute_composite_similarity_scores(us_search_responses_df)

print(us_search_responses_df_with_composite.head())

"""# Run t-tests on composite similarity scores

## Run t-tests
"""

import pandas as pd
from scipy.stats import ttest_ind
import itertools
from IPython.display import display

def run_pairwise_t_tests(df):
    similarity_cols = [
        "USHMM-GPT Similarity",
        "USHMM-Gemini Similarity",
        "USHMM-Grok Similarity",
        "GPT-Gemini Similarity",
        "GPT-Grok Similarity",
        "Gemini-Grok Similarity"
    ]

    t_test_results = []

    # Perform pairwise t-tests between all combinations of similarity scores
    for col1, col2 in itertools.combinations(similarity_cols, 2):
        scores1 = df[col1].dropna()
        scores2 = df[col2].dropna()

        if len(scores1) > 1 and len(scores2) > 1:
            t_stat, p_val = ttest_ind(scores1, scores2, equal_var=False)
            t_test_results.append({
                "Comparison": f"{col1} vs {col2}",
                "Mean 1": scores1.mean(),
                "Mean 2": scores2.mean(),
                "t-statistic": round(t_stat, 4),
                "p-value": round(p_val, 4),
                "Significant (p < 0.05)": p_val < 0.05
            })
        else:
            t_test_results.append({
                "Comparison": f"{col1} vs {col2}",
                "Mean 1": scores1.mean() if len(scores1) > 0 else None,
                "Mean 2": scores2.mean() if len(scores2) > 0 else None,
                "t-statistic": None,
                "p-value": None,
                "Significant (p < 0.05)": None
            })

    results_df = pd.DataFrame(t_test_results)
    display(results_df)
    return results_df

results_df = run_pairwise_t_tests(us_search_responses_df_with_composite)

us_search_responses_df_with_composite

import pandas as pd
from scipy.stats import f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import statsmodels.api as sm

def run_anova_and_tukey(df, similarity_columns, perform_tukey=True):
    """
    Runs one-way ANOVA across multiple similarity score columns and optional Tukey HSD test.

    Parameters:
        df (pd.DataFrame): The DataFrame containing similarity scores.
        similarity_columns (list): List of column names to compare.
        perform_tukey (bool): Whether to perform Tukey's HSD post-hoc test if ANOVA is significant.

    Returns:
        dict: Contains ANOVA result and (optionally) Tukey HSD result.
    """
    # Extract data for ANOVA
    groups = [df[col].dropna().values for col in similarity_columns]

    # Run one-way ANOVA
    anova_result = f_oneway(*groups)
    result = {
        "anova_p_value": anova_result.pvalue
    }

    # Optional: Tukey HSD post-hoc test
    if perform_tukey and anova_result.pvalue < 0.05:
        # Reshape to long format
        df_long = df.melt(value_vars=similarity_columns, var_name="Comparison", value_name="Similarity")
        tukey = pairwise_tukeyhsd(endog=df_long["Similarity"], groups=df_long["Comparison"], alpha=0.05)
        result["tukey_summary"] = tukey.summary()

    return result

similarity_cols = [
    "USHMM-GPT Similarity",
    "USHMM-Gemini Similarity",
    "USHMM-Grok Similarity",
    "GPT-Gemini Similarity",
    "GPT-Grok Similarity",
    "Gemini-Grok Similarity"
]

results = run_anova_and_tukey(us_search_responses_df_with_composite, similarity_cols)

print("ANOVA p-value:", results["anova_p_value"])

if "tukey_summary" in results:
    print("\nTukey HSD Results:")
    print(results["tukey_summary"])

import pandas as pd
import itertools
import numpy as np

def cohen_d(x, y):
    """Calculate Cohen's d for two independent samples."""
    x, y = np.array(x), np.array(y)
    nx, ny = len(x), len(y)
    pooled_std = np.sqrt(((x.std(ddof=1) ** 2) + (y.std(ddof=1) ** 2)) / 2)
    return (x.mean() - y.mean()) / pooled_std

def calculate_pairwise_cohens_d(df, similarity_columns):
    """
    Calculates Cohen's d for all pairwise combinations of similarity columns.

    Parameters:
        df (pd.DataFrame): DataFrame containing similarity score columns.
        similarity_columns (list): List of column names to compare.

    Returns:
        pd.DataFrame: A table of pairwise comparisons and their Cohen's d values.
    """
    results = []

    for col1, col2 in itertools.combinations(similarity_columns, 2):
        scores1 = df[col1].dropna()
        scores2 = df[col2].dropna()
        d = cohen_d(scores1, scores2)
        results.append({
            "Comparison": f"{col1} vs {col2}",
            "Mean 1": scores1.mean(),
            "Mean 2": scores2.mean(),
            "Cohen's d": round(d, 4),
            "Effect Size": interpret_cohens_d(d)
        })

    return pd.DataFrame(results)

def interpret_cohens_d(d):
    """Categorize effect size based on Cohen's d."""
    d = abs(d)
    if d < 0.2:
        return "Negligible"
    elif d < 0.5:
        return "Small"
    elif d < 0.8:
        return "Medium"
    else:
        return "Large"

cohen_results = calculate_pairwise_cohens_d(us_search_responses_df_with_composite, similarity_cols)
print(cohen_results)